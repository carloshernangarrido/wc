{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import os\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_images = r'C:\\Fotos y videos\\test bovw'\n",
    "path_images = r'C:\\TRABAJO\\Willdom\\Sr Machine Learning Engineer\\Challenge'\n",
    "working_resolution = [512,512]\n",
    "vocabulary_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def load_images_from_folder(folder: str, resolution: list) -> list:\n",
    "    images = {}\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.split('.')[-1] == 'jpg':\n",
    "            path = os.path.join(folder, filename)\n",
    "            img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "            img = cv2.resize(img, resolution)\n",
    "            if img is not None:\n",
    "                images[filename] = img\n",
    "    return images\n",
    "\n",
    "def sift_dictionary(images: dict) -> list:\n",
    "    \"\"\"Create a dictionary of descriptors with SIFT.\"\"\"\n",
    "    sift = cv2.SIFT_create()\n",
    "    visual_dictionary = []\n",
    "    descriptors_by_image = {}\n",
    "    for name, img in images.items():\n",
    "        kp, des = sift.detectAndCompute(img, None)\n",
    "        # print(f'image {name} has {len(des)} descriptors')\n",
    "        visual_dictionary.extend(des)\n",
    "        descriptors_by_image[name] = des\n",
    "    return visual_dictionary, descriptors_by_image\n",
    "\n",
    "def kmeans_visual_vocabulary(k: int, visual_dictionary: list) -> tuple:\n",
    "    \"\"\"Create a vocabulary with k-means quantization.\"\"\"\n",
    "    kmeans = KMeans(n_init=10, n_clusters=64)\n",
    "    kmeans.fit(visual_dictionary)\n",
    "    visual_words = kmeans.cluster_centers_\n",
    "    word_classifier = lambda x: kmeans.predict(np.array(x, dtype='float')) \n",
    "    return visual_words, word_classifier\n",
    "\n",
    "def get_words_by_image(descriptors_by_image: dict, word_classifier) -> dict:\n",
    "    \"\"\"Get the visual words in each image by using kmeans predictor.\"\"\"\n",
    "    words_by_image = {}\n",
    "    for name, descriptors in descriptors_by_image.items():\n",
    "        words_by_image[name] = word_classifier(descriptors)\n",
    "    return words_by_image\n",
    "\n",
    "def get_global_descriptor_by_image(words_by_image: dict, vocabulary_size: int) -> Tuple[dict, np.array]:\n",
    "    \"\"\"Create a histogram of visual words for each image\"\"\"\n",
    "    global_descriptor_by_image = {}\n",
    "    for name, words in words_by_image.items():\n",
    "        global_descriptor_by_image[name] = np.array(\n",
    "            np.histogram(words, range=(-.5,vocabulary_size+.5), bins=vocabulary_size)[0],\n",
    "        dtype='float32')\n",
    "\n",
    "    # global_descriptor_by_image to array of descriptors\n",
    "    global_descriptors = np.zeros((len(words_by_image), vocabulary_size), \n",
    "                                    dtype=global_descriptor_by_image[list(global_descriptor_by_image.keys())[0]].dtype)\n",
    "    i = 0\n",
    "    for name, global_descriptor in global_descriptor_by_image.items():\n",
    "        global_descriptors[i, :] = global_descriptor_by_image[name]\n",
    "        i += 1\n",
    "\n",
    "    return global_descriptor_by_image, global_descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageRetriever:\n",
    "    def __init__(self, working_resolution: list = None, vocabulary_size: int = 64) -> None:\n",
    "        self.working_resolution = [512, 512] if working_resolution is None else working_resolution\n",
    "        assert len(self.working_resolution) == 2\n",
    "        assert isinstance(vocabulary_size, int) and vocabulary_size > 2\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "\n",
    "        self.images = None\n",
    "        self.visual_dictionary = None\n",
    "        self.descriptors_by_image = None\n",
    "        self.visual_words = None\n",
    "        self.word_classifier = None\n",
    "        self.words_by_image = None\n",
    "        self.global_descriptor_by_image = None\n",
    "        self.global_descriptors = None\n",
    "\n",
    "    def fit(self, folder_path: str):     \n",
    "        # Processing\n",
    "        print('Loading images...')        \n",
    "        self.images = load_images_from_folder(folder_path, self.working_resolution)\n",
    "        print(f'{len(self.images)} images were loaded')\n",
    "\n",
    "        print('Extracting local descriptors from the images...')   \n",
    "        self.visual_dictionary, self.descriptors_by_image = sift_dictionary(self.images)\n",
    "        print(f'The visual dictionary has {len(self.visual_dictionary)} descriptors')\n",
    "\n",
    "        print('Clustering local descriptors...')\n",
    "        self.visual_words, self.word_classifier = kmeans_visual_vocabulary(self.vocabulary_size, self.visual_dictionary)\n",
    "        print(f'The visual vocabulary has {len(self.visual_words)} visual words')\n",
    "\n",
    "        print('Estracting visual words from the image local descriptors...')\n",
    "        self.words_by_image = get_words_by_image(self.descriptors_by_image, self.word_classifier)\n",
    "        print('The visual words of each image are ready.')\n",
    "\n",
    "        print(\"Computing a global descriptor for each image...\")\n",
    "        self.global_descriptor_by_image, self.global_descriptors = get_global_descriptor_by_image(self.words_by_image, self.vocabulary_size)\n",
    "        print(\"The global descriptors of each image are ready.\")\n",
    "\n",
    "    def retrieve_from_training_set(self, query_image_name: str, result_size: int = 3,\n",
    "                                   display_image_results: bool = True) -> (list, list):\n",
    "        # Create a matcher\n",
    "        bf = cv2.FlannBasedMatcher()\n",
    "\n",
    "        # Match descriptors\n",
    "        matches = bf.knnMatch(self.global_descriptor_by_image[query_image_name].reshape((1, -1)), \n",
    "                              self.global_descriptors, k=result_size)[0]\n",
    "        matches_idx = [match.trainIdx for match in matches]\n",
    "        print(matches_idx)\n",
    "        matches_names = [list(self.images.keys())[i] for i in matches_idx]\n",
    "        print(matches_names)\n",
    "\n",
    "        if display_image_results:\n",
    "            for i in range(result_size):\n",
    "                name = matches_names[i]\n",
    "                cv2.imshow(f'Query result: {name} - Distance: {matches[i].distance}', cv2.resize(cv2.imread(name), working_resolution))\n",
    "            cv2.waitKey(0)\n",
    "            cv2.destroyAllWindows()\n",
    "        \n",
    "        return matches_names, matches\n",
    "    \n",
    "    def retrieve_from_file(self, filename: str, result_size: int = 3,\n",
    "                           display_image_results: bool = True) -> (list, list):\n",
    "        # load query image\n",
    "        assert filename.split('.')[-1] == 'jpg'\n",
    "        query_image = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)\n",
    "        query_image = cv2.resize(query_image, self.working_resolution)\n",
    "        query_image_name = filename\n",
    "        query_images = {query_image_name: query_image}\n",
    "\n",
    "        cv2.imshow('Query image', query_image)\n",
    "\n",
    "        # Get global descriptor of query_image\n",
    "        _, descriptors_in_query_image = sift_dictionary(query_images)\n",
    "        words_in_query_image = get_words_by_image(descriptors_in_query_image, self.word_classifier)\n",
    "        _, global_descriptor_of_query_image = get_global_descriptor_by_image(words_in_query_image, self.vocabulary_size)\n",
    "\n",
    "        # Create a matcher\n",
    "        bf = cv2.FlannBasedMatcher()\n",
    "\n",
    "        # Match descriptors\n",
    "        matches = bf.knnMatch(global_descriptor_of_query_image, \n",
    "                              self.global_descriptors, k=result_size)[0]\n",
    "        matches_idx = [match.trainIdx for match in matches]\n",
    "        print(matches_idx)\n",
    "        matches_names = [list(self.images.keys())[i] for i in matches_idx]\n",
    "        print(matches_names)\n",
    "\n",
    "        if display_image_results:\n",
    "            for i in range(result_size):\n",
    "                name = matches_names[i]\n",
    "                cv2.imshow(f'Query result: {name} - Distance: {matches[i].distance}', cv2.resize(cv2.imread(name), working_resolution))\n",
    "            cv2.waitKey(0)\n",
    "        \n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "        return matches_names, matches\n",
    "\n",
    "    def display_images(self):\n",
    "        if self.images is not None:\n",
    "            cv2.namedWindow('image', cv2.WINDOW_GUI_EXPANDED)\n",
    "            for filename in self.images.keys():\n",
    "                cv2.imshow('image', self.images[filename])\n",
    "                cv2.waitKey(0)\n",
    "            cv2.destroyAllWindows()\n",
    "        else:\n",
    "            print('Images not loaded, use .fit()')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images...\n",
      "28 images were loaded\n",
      "Extracting local descriptors from the images...\n",
      "The visual dictionary has 37999 descriptors\n",
      "Clustering local descriptors...\n",
      "The visual vocabulary has 64 visual words\n",
      "Estracting visual words from the image local descriptors...\n",
      "The visual words of each image are ready.\n",
      "Computing a global descriptor for each image...\n",
      "The global descriptors of each image are ready.\n"
     ]
    }
   ],
   "source": [
    "ir = ImageRetriever()\n",
    "\n",
    "ir.fit(path_images)\n",
    "ir.display_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On an image included in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 9, 11, 8, 13, 12, 6, 10, 22, 23]\n",
      "['IMG_20220203_152013349.jpg', 'IMG_20220214_144907525.jpg', 'IMG_20220216_143433424.jpg', 'IMG_20220214_134233140.jpg', 'IMG_20220216_171549908.jpg', 'IMG_20220216_145949270.jpg', 'IMG_20220214_124603956.jpg', 'IMG_20220215_144642554.jpg', 'IMG_20231217_222533482.jpg', 'IMG_20231217_222547646_HDR.jpg']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['IMG_20220203_152013349.jpg',\n",
       "  'IMG_20220214_144907525.jpg',\n",
       "  'IMG_20220216_143433424.jpg',\n",
       "  'IMG_20220214_134233140.jpg',\n",
       "  'IMG_20220216_171549908.jpg',\n",
       "  'IMG_20220216_145949270.jpg',\n",
       "  'IMG_20220214_124603956.jpg',\n",
       "  'IMG_20220215_144642554.jpg',\n",
       "  'IMG_20231217_222533482.jpg',\n",
       "  'IMG_20231217_222547646_HDR.jpg'],\n",
       " (< cv2.DMatch 000001F89A548670>,\n",
       "  < cv2.DMatch 000001F89A548F70>,\n",
       "  < cv2.DMatch 000001F89A548510>,\n",
       "  < cv2.DMatch 000001F89A5485D0>,\n",
       "  < cv2.DMatch 000001F89A5489F0>,\n",
       "  < cv2.DMatch 000001F89A5482D0>,\n",
       "  < cv2.DMatch 000001F89A5485F0>,\n",
       "  < cv2.DMatch 000001F89A5485B0>,\n",
       "  < cv2.DMatch 000001F89A548770>,\n",
       "  < cv2.DMatch 000001F89A548050>))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ir.retrieve_from_training_set('IMG_20220203_152013349.jpg', result_size=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On an unseen image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 17, 15]\n",
      "['IMG_20231202_090817_1068008126061804504.jpg', 'IMG_20220330_143722955.jpg', 'IMG_20220330_143611689.jpg']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['IMG_20231202_090817_1068008126061804504.jpg',\n",
       "  'IMG_20220330_143722955.jpg',\n",
       "  'IMG_20220330_143611689.jpg'],\n",
       " (< cv2.DMatch 000001F89A091550>,\n",
       "  < cv2.DMatch 000001F89A5480B0>,\n",
       "  < cv2.DMatch 000001F89A548070>))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ir.retrieve_from_file(r'C:\\TRABAJO\\Willdom\\Sr Machine Learning Engineer\\Challenge\\query images\\IMG_20230829_185501327.jpg', \n",
    "                      result_size=3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "car_models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
